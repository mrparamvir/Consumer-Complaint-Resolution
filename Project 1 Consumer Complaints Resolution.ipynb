{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumers who are more likely to dispute a conclusion\n",
    "\n",
    "In this particular case we have been given detailed consumer complaints along with whether consumer disputed with the conclusion. If we are able to predict this, consumer who is more likely to dispute a conclusion can be given more attention as to how the complaints are handled as well as how persuasively the final conlusions are conveyed to them.\n",
    "\n",
    "We're going to take the following approach:\n",
    "1. Problem definition\n",
    "2. Data\n",
    "3. Evaluation\n",
    "4. Modelling\n",
    "\n",
    "## 1. Problem Definition\n",
    "\n",
    "In a statement,\n",
    "> Given a detailed consumer complaints, can we predict whether or not a customer is going to dispute?\n",
    "\n",
    "## 2. Data\n",
    "\n",
    "For training the model : `Consumer_Complaints_train.csv`\n",
    "For testing the model: `Consumer_Complaints_test_share.csv`\n",
    "\n",
    "## 3. Evaluation\n",
    "\n",
    "> If we can get atleast 0.54 AUC score at predicting whether or not a customer will dispute or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the tools\n",
    "\n",
    "We're going to use `pandas`, `matplotlib`, `scikit-learn` and `numpy` for data analysis and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Consumer_Complaints_train.csv')\n",
    "df_test = pd.read_csv('Consumer_Complaints_test_share.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74019"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['Consumer complaint narrative'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 478421 entries, 0 to 478420\n",
      "Data columns (total 18 columns):\n",
      " #   Column                        Non-Null Count   Dtype \n",
      "---  ------                        --------------   ----- \n",
      " 0   Date received                 478421 non-null  object\n",
      " 1   Product                       478421 non-null  object\n",
      " 2   Sub-product                   339948 non-null  object\n",
      " 3   Issue                         478421 non-null  object\n",
      " 4   Sub-issue                     185796 non-null  object\n",
      " 5   Consumer complaint narrative  75094 non-null   object\n",
      " 6   Company public response       90392 non-null   object\n",
      " 7   Company                       478421 non-null  object\n",
      " 8   State                         474582 non-null  object\n",
      " 9   ZIP code                      474573 non-null  object\n",
      " 10  Tags                          67206 non-null   object\n",
      " 11  Consumer consent provided?    135487 non-null  object\n",
      " 12  Submitted via                 478421 non-null  object\n",
      " 13  Date sent to company          478421 non-null  object\n",
      " 14  Company response to consumer  478421 non-null  object\n",
      " 15  Timely response?              478421 non-null  object\n",
      " 16  Consumer disputed?            478421 non-null  object\n",
      " 17  Complaint ID                  478421 non-null  int64 \n",
      "dtypes: int64(1), object(17)\n",
      "memory usage: 65.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()\n",
    "\n",
    "#dropping -> Sub issue, Complaint narrative, Public response, Zip, Tag\n",
    "\n",
    "df_train.drop(['Sub-issue','Consumer complaint narrative','Company public response',\n",
    "               'ZIP code','Tags','Consumer consent provided?']\n",
    "               ,1,inplace=True)\n",
    "\n",
    "#dropping -> Sub issue, Complaint narrative, Public response, Zip, Tag\n",
    "\n",
    "df_test.drop(['Sub-issue','Consumer complaint narrative','Company public response',\n",
    "               'ZIP code','Tags','Consumer consent provided?']\n",
    "               ,1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Dates\n",
    "# Date received and date sent to company\n",
    "\n",
    "df_train['Date received'] = pd.to_datetime(df_train['Date received'], errors='coerce',infer_datetime_format = True) \n",
    "\n",
    "df_train['Date sent to company']=pd.to_datetime(df_train['Date sent to company'], errors='coerce',infer_datetime_format = True)\n",
    "\n",
    "df_train['days_diff'] = df_train['Date sent to company'] - df_train['Date received']\n",
    "\n",
    "df_train.drop(['Date received','Date sent to company'],1,inplace=True)\n",
    "\n",
    "df_train['days_diff'] = pd.to_numeric(df_train['days_diff'].dt.days, downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date received and date sent to company\n",
    "df_test['Date received'] = pd.to_datetime(df_test['Date received'], errors='coerce',infer_datetime_format = True) \n",
    "\n",
    "df_test['Date sent to company']=pd.to_datetime(df_test['Date sent to company'], errors='coerce',infer_datetime_format = True)\n",
    "\n",
    "df_test['days_diff'] = df_test['Date sent to company'] - df_test['Date received']\n",
    "\n",
    "df_test.drop(['Date received','Date sent to company'],1,inplace=True)\n",
    "\n",
    "df_test['days_diff'] = pd.to_numeric(df_test['days_diff'].dt.days, downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Consumer disputed?'].value_counts()\n",
    "\n",
    "df_train['Y'] = np.where(df_train['Consumer disputed?'] == 'Yes',1,0)\n",
    "del df_train['Consumer disputed?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product\n",
      "Submitted via\n",
      "Company response to consumer\n",
      "Timely response?\n"
     ]
    }
   ],
   "source": [
    "#product, submitted via, company response, timely response\n",
    "conv_dummies = ['Product','Submitted via','Company response to consumer','Timely response?']\n",
    "\n",
    "for col in conv_dummies:\n",
    "    dummy=pd.get_dummies(df_train[col],prefix=col,drop_first=True)\n",
    "    df_train=pd.concat([df_train,dummy],axis=1)\n",
    "    print(col)\n",
    "    del df_train[col]\n",
    "del dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product\n",
      "Submitted via\n",
      "Company response to consumer\n",
      "Timely response?\n"
     ]
    }
   ],
   "source": [
    "#product, submitted via, company response, timely response\n",
    "conv_dummies = ['Product','Submitted via','Company response to consumer','Timely response?']\n",
    "\n",
    "for col in conv_dummies:\n",
    "    dummy=pd.get_dummies(df_test[col],prefix=col,drop_first=True)\n",
    "    df_test=pd.concat([df_test,dummy],axis=1)\n",
    "    print(col)\n",
    "    del df_test[col]\n",
    "del dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=df_train['State'].value_counts()\n",
    "for val in k.axes[0][0:15]:\n",
    "    varname='State_'+val.replace(',','_').replace(' ','_')\n",
    "    df_train[varname]=np.where(df_train['State']==val,1,0)\n",
    "    df_test[varname]=np.where(df_test['State']==val,1,0)\n",
    "del df_train['State']\n",
    "del df_test['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=df_train['Company'].value_counts()\n",
    "for val in k.axes[0][0:40]:\n",
    "    varname='Company_'+val.replace(',','_').replace(' ','_')\n",
    "    df_train[varname]=np.where(df_train['Company']==val,1,0)\n",
    "    df_test[varname]=np.where(df_test['Company']==val,1,0)\n",
    "del df_train['Company']\n",
    "del df_test['Company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=df_train['Issue'].value_counts()\n",
    "for val in k.axes[0][0:30]:\n",
    "    varname='Issue_'+val.replace(',','_').replace(' ','_')\n",
    "    df_train[varname]=np.where(df_train['Issue']==val,1,0)\n",
    "    df_test[varname]=np.where(df_test['Issue']==val,1,0)\n",
    "del df_train['Issue']\n",
    "del df_test['Issue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=df_train['Sub-product'].value_counts()\n",
    "for val in k.axes[0][0:30]:\n",
    "    varname='Sub-product_'+val.replace(',','_').replace(' ','_')\n",
    "    df_train[varname]=np.where(df_train['Sub-product']==val,1,0)\n",
    "    df_test[varname]=np.where(df_test['Sub-product']==val,1,0)\n",
    "del df_train['Sub-product']\n",
    "del df_test['Sub-product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(inplace=True)\n",
    "\n",
    "df_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df_train.drop(['Y','Complaint ID'],1)\n",
    "y=df_train['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=np.where(clf.predict(df_test.drop(['Complaint ID'],1))==1,\"Yes\",\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame(list(zip(df_test['Complaint ID'],list(prediction))),\n",
    "                       columns=['Complaint ID','Consumer disputed?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Complaint ID</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>675956</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1858795</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32637</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1731374</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Complaint ID Consumer disputed?\n",
       "0        675956                 No\n",
       "1       1858795                 No\n",
       "2         32637                 No\n",
       "3       1731374                 No"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.304956  , 0.21837561, 0.09678504, ..., 0.24199153, 0.29827327,\n",
       "       0.15781488])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train score\n",
    "train_score=clf.predict_proba(x)[:,1] \n",
    "\n",
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30636425, 0.22085381, 0.34360414, ..., 0.19834864, 0.21275703,\n",
       "       0.21427519])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test score\n",
    "x_test=df_test.drop('Complaint ID',1)\n",
    "\n",
    "test_score=clf.predict_proba(x_test)[:,1] \n",
    "\n",
    "test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df_train.drop(['Y','Complaint ID'],1)\n",
    "y=df_train['Y']\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001   0.6173063220944757\n",
      "0.01   0.6208255734373733\n",
      "0.1   0.6215246635891455\n",
      "1.0   0.6210660249567241\n",
      "10   0.6210952280830376\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "logr_ht=LogisticRegression()\n",
    "logr_ht\n",
    "roc_dict={}\n",
    "\n",
    "c_range=[0.001,0.01,0.1,1.0,10]\n",
    "\n",
    "for i in c_range:\n",
    "    logr_ht=LogisticRegression(C=i)\n",
    "    logr_ht.fit(x_train, y_train)\n",
    "    pred=logr_ht.predict_proba(x_test)[:,1]\n",
    "    r1=roc_auc_score(y_test, pred)\n",
    "    roc_dict[i]=r1\n",
    "    print(i , ' ' ,  roc_auc_score(y_test, pred))\n",
    "\n",
    "\n",
    "roc_dict\n",
    "\n",
    "Keymax = max(roc_dict, key=roc_dict.get) \n",
    "print(Keymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=LogisticRegression(C=0.1)\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Complaint ID</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>675956</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1858795</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32637</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1731374</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Complaint ID Consumer disputed?\n",
       "0        675956                 No\n",
       "1       1858795                 No\n",
       "2         32637                 No\n",
       "3       1731374                 No"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction=np.where(clf.predict(df_test.drop(['Complaint ID'],1))==1,\"Yes\",\"No\")\n",
    "\n",
    "submission=pd.DataFrame(list(zip(df_test['Complaint ID'],list(prediction))),\n",
    "                       columns=['Complaint ID','Consumer disputed?'])\n",
    "\n",
    "submission.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30051791, 0.21698454, 0.36007386, ..., 0.195078  , 0.20972367,\n",
       "       0.21230586])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.to_csv('Project1_submission_classes.csv',index=False)\n",
    "\n",
    "#train score\n",
    "train_score=clf.predict_proba(x)[:,1] \n",
    "\n",
    "train_score\n",
    "\n",
    "#test score\n",
    "x_test=df_test.drop('Complaint ID',1)\n",
    "\n",
    "test_score=clf.predict_proba(x_test)[:,1] \n",
    "\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_score).to_csv(\"Project1_submission_score.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
